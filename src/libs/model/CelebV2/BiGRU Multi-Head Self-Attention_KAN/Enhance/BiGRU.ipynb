{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecb6a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r /home/huuquangdang/huu.quang.dang/thesis/deepfake/deepfake_backend/libs/model/CelebV2/requirements.txt\n",
    "\n",
    "# # Step 1: Uninstall incompatible NumPy version\n",
    "# %pip uninstall numpy -y\n",
    "# # Step 2: Install compatible NumPy version (< 2.0.0)\n",
    "# %pip install \"numpy>=1.21.0,<2.0.0\"\n",
    "# # Step 3: Verify NumPy version\n",
    "# import numpy as np\n",
    "# print(f\"âœ… NumPy version: {np.__version__}\")\n",
    "# print(f\"Expected: < 2.0.0 (you should see 1.x.x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "149fd0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:44:38.582738Z",
     "start_time": "2025-08-13T09:44:38.568745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, TimeDistributed, GlobalAveragePooling2D, LSTM, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46ecf6429cdc7d99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:06:51.462172Z",
     "start_time": "2025-06-16T15:06:51.281795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "base_path = '/home/huuquangdang/huu.quang.dang/thesis/Dataset/celeb_df_crop'\n",
    "categories = ['fake', 'real']\n",
    "\n",
    "# Initialize a list to hold data\n",
    "data = []\n",
    "\n",
    "# Process each category\n",
    "for category in categories:\n",
    "    category_path = os.path.join(base_path, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            try:\n",
    "                id_part, frame_part = filename.split('_frame_')\n",
    "                id_ = id_part.split('_')[0]\n",
    "                frame = frame_part.split('.')[0]\n",
    "                data.append({\n",
    "                    'filename': filename,\n",
    "                    'path': os.path.join(category_path, filename),\n",
    "                    'id': int(id_),\n",
    "                    'frame': int(frame),\n",
    "                    'label': category\n",
    "                })\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc17e5b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:06:53.591689Z",
     "start_time": "2025-06-16T15:06:53.575495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>frame</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264_id37_0009_frame_0001.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>271_id38_0006_frame_0014.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>271</td>\n",
       "      <td>14</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>015_id10_0006_frame_0027.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>392_id4_0007_frame_0028.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>392</td>\n",
       "      <td>28</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029_id11_0010_frame_0004.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32367</th>\n",
       "      <td>475_id58_0004_frame_0012.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>475</td>\n",
       "      <td>12</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32368</th>\n",
       "      <td>014_id10_0005_frame_0012.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32369</th>\n",
       "      <td>365_id48_0000_frame_0023.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>365</td>\n",
       "      <td>23</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32370</th>\n",
       "      <td>091_id20_0000_frame_0026.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>91</td>\n",
       "      <td>26</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32371</th>\n",
       "      <td>356_id47_0001_frame_0019.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>356</td>\n",
       "      <td>19</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32372 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0      264_id37_0009_frame_0001.jpg   \n",
       "1      271_id38_0006_frame_0014.jpg   \n",
       "2      015_id10_0006_frame_0027.jpg   \n",
       "3       392_id4_0007_frame_0028.jpg   \n",
       "4      029_id11_0010_frame_0004.jpg   \n",
       "...                             ...   \n",
       "32367  475_id58_0004_frame_0012.jpg   \n",
       "32368  014_id10_0005_frame_0012.jpg   \n",
       "32369  365_id48_0000_frame_0023.jpg   \n",
       "32370  091_id20_0000_frame_0026.jpg   \n",
       "32371  356_id47_0001_frame_0019.jpg   \n",
       "\n",
       "                                                    path   id  frame label  \\\n",
       "0      /home/huuquangdang/huu.quang.dang/thesis/Datas...  264      1  fake   \n",
       "1      /home/huuquangdang/huu.quang.dang/thesis/Datas...  271     14  fake   \n",
       "2      /home/huuquangdang/huu.quang.dang/thesis/Datas...   15     27  fake   \n",
       "3      /home/huuquangdang/huu.quang.dang/thesis/Datas...  392     28  fake   \n",
       "4      /home/huuquangdang/huu.quang.dang/thesis/Datas...   29      4  fake   \n",
       "...                                                  ...  ...    ...   ...   \n",
       "32367  /home/huuquangdang/huu.quang.dang/thesis/Datas...  475     12  real   \n",
       "32368  /home/huuquangdang/huu.quang.dang/thesis/Datas...   14     12  real   \n",
       "32369  /home/huuquangdang/huu.quang.dang/thesis/Datas...  365     23  real   \n",
       "32370  /home/huuquangdang/huu.quang.dang/thesis/Datas...   91     26  real   \n",
       "32371  /home/huuquangdang/huu.quang.dang/thesis/Datas...  356     19  real   \n",
       "\n",
       "       label_id  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "32367         1  \n",
       "32368         1  \n",
       "32369         1  \n",
       "32370         1  \n",
       "32371         1  \n",
       "\n",
       "[32372 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Äáº£m báº£o Ä‘Ã£ cÃ³ df_cropped.csv chá»©a Ä‘Æ°á»ng dáº«n áº£nh Ä‘Ã£ crop\n",
    "df['label_id'] = df['label'].map({'fake': 0, 'real': 1})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c24e6874-6fe8-4dff-9f0d-5c7d94337ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:06:56.671685Z",
     "start_time": "2025-06-16T15:06:55.874053Z"
    }
   },
   "outputs": [],
   "source": [
    "df['video_key'] = df['id'].astype(str) + \"_\" + df['label']\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "video_dict = defaultdict(list)\n",
    "labels = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    key = row['video_key']\n",
    "    video_dict[key].append(row['path'])\n",
    "    labels[key] = row['label_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc8891d0-4a1f-4bcd-97f2-0b9577706cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T06:50:19.887250Z",
     "start_time": "2025-06-16T06:50:19.279020Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "016d786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading pre-extracted OpenFace features from CSV files...\n",
      "âœ… Loaded 16223 feature vectors\n",
      "âœ… Each vector has 674 dimensions\n",
      "âœ… Feature dictionary ready for 1-to-1 frame mapping\n",
      "âœ… Loaded 16223 feature vectors\n",
      "âœ… Each vector has 674 dimensions\n",
      "âœ… Feature dictionary ready for 1-to-1 frame mapping\n"
     ]
    }
   ],
   "source": [
    "# Load pre-extracted CSV features for feature fusion\n",
    "print(\"ðŸ“‚ Loading pre-extracted OpenFace features from CSV files...\")\n",
    "csv_fake_path = '/home/huuquangdang/huu.quang.dang/thesis/deepfake/deepfake_backend/libs/tools/fake/op_vectors_fake_v1.csv'\n",
    "csv_real_path = '/home/huuquangdang/huu.quang.dang/thesis/deepfake/deepfake_backend/libs/tools/real/op_vectors_real_v1.csv'\n",
    "\n",
    "df_fake_features = pd.read_csv(csv_fake_path, header=None)\n",
    "df_real_features = pd.read_csv(csv_real_path, header=None)\n",
    "\n",
    "# Combine both dataframes\n",
    "df_all_features = pd.concat([df_fake_features, df_real_features], ignore_index=True)\n",
    "df_all_features.columns = ['filename'] + [f'feat_{i}' for i in range(df_all_features.shape[1] - 1)]\n",
    "\n",
    "# Create a dictionary for fast lookup: filename -> features\n",
    "openface_features = {}\n",
    "for _, row in df_all_features.iterrows():\n",
    "    filename = row['filename']\n",
    "    features = row.iloc[1:].values.astype('float32')\n",
    "    openface_features[filename] = features\n",
    "\n",
    "csv_feature_dim = len(features)\n",
    "print(f\"âœ… Loaded {len(openface_features)} feature vectors\")\n",
    "print(f\"âœ… Each vector has {csv_feature_dim} dimensions\")\n",
    "print(f\"âœ… Feature dictionary ready for 1-to-1 frame mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20b762d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Debugging filename matching:\n",
      "\n",
      "ðŸ“ Sample image filenames (from video_dict):\n",
      "   - 264_id37_0009_frame_0001.jpg\n",
      "   - 264_id37_0009_frame_0006.jpg\n",
      "   - 264_id37_0009_frame_0008.jpg\n",
      "\n",
      "ðŸ“„ Sample CSV feature keys (from openface_features):\n",
      "   - 000_id0_0000_frame_0000.jpg\n",
      "   - 000_id0_0000_frame_0001.jpg\n",
      "   - 000_id0_0000_frame_0002.jpg\n",
      "   - 000_id0_0000_frame_0003.jpg\n",
      "   - 000_id0_0000_frame_0004.jpg\n",
      "\n",
      "ðŸ”Ž Checking for matches:\n",
      "   âœ… MATCH: 264_id37_0009_frame_0001.jpg\n",
      "   âœ… MATCH: 264_id37_0009_frame_0006.jpg\n",
      "   âœ… MATCH: 264_id37_0009_frame_0008.jpg\n",
      "\n",
      "ðŸ“Š Total matches: 3/3\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” Debug: Check filename matching between images and CSV features\n",
    "print(\"ðŸ” Debugging filename matching:\")\n",
    "print(f\"\\nðŸ“ Sample image filenames (from video_dict):\")\n",
    "sample_paths = list(video_dict.values())[0][:3]\n",
    "for path in sample_paths:\n",
    "    print(f\"   - {os.path.basename(path)}\")\n",
    "\n",
    "print(f\"\\nðŸ“„ Sample CSV feature keys (from openface_features):\")\n",
    "sample_csv_keys = list(openface_features.keys())[:5]\n",
    "for key in sample_csv_keys:\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "# Check for matches\n",
    "print(f\"\\nðŸ”Ž Checking for matches:\")\n",
    "matches = 0\n",
    "for path in sample_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    if filename in openface_features:\n",
    "        print(f\"   âœ… MATCH: {filename}\")\n",
    "        matches += 1\n",
    "    else:\n",
    "        print(f\"   âŒ NO MATCH: {filename}\")\n",
    "        # Try to find similar names\n",
    "        similar = [k for k in list(openface_features.keys())[:10] if filename[:10] in k or k[:10] in filename]\n",
    "        if similar:\n",
    "            print(f\"      Similar: {similar[:3]}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total matches: {matches}/{len(sample_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfba17a",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Model: BiGRU + Multi-Head Self-Attention\n",
    "\n",
    "This cell implements **BiGRU with Multi-Head Self-Attention mechanism** for improved sequence modeling, while maintaining:\n",
    "- âœ… Same function names (`VideoSequence`, `build_model`, `hmm_postprocess`)\n",
    "- âœ… Same 1-to-1 feature fusion (MobileNet + CSV)\n",
    "- âœ… Same normalization strategy\n",
    "- âœ… Enhanced with Bidirectional GRU and Multi-Head Self-Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Computing CSV feature statistics for normalization...\n",
      "ðŸ“Š Found 16253 CSV features from 1084 videos\n",
      "âœ… CSV feature normalization fitted on 16253 samples\n",
      "\n",
      "ðŸš€ Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\n",
      "   âœ… Bidirectional GRU (256 + 128 units) with recurrent dropout\n",
      "   âœ… Dual Multi-Head Self-Attention (8 heads + 4 heads)\n",
      "   âœ… Layer Normalization after each major block\n",
      "   âœ… Enhanced data augmentation\n",
      "   âœ… Sequence length: 15 frames\n",
      "   âœ… MobileNet (last 30 layers trainable)\n",
      "   âœ… Stronger regularization (dropout 0.3-0.4)\n",
      "   âœ… HMM post-processing\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Fold 1/5 - Enhanced BiGRU_MultiHeadAttn\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.5557\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Computing CSV feature statistics for normalization...\n",
      "ðŸ“Š Found 16253 CSV features from 1084 videos\n",
      "âœ… CSV feature normalization fitted on 16253 samples\n",
      "\n",
      "ðŸš€ Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\n",
      "   âœ… Bidirectional GRU (256 + 128 units) with recurrent dropout\n",
      "   âœ… Dual Multi-Head Self-Attention (8 heads + 4 heads)\n",
      "   âœ… Layer Normalization after each major block\n",
      "   âœ… Enhanced data augmentation\n",
      "   âœ… Sequence length: 15 frames\n",
      "   âœ… MobileNet (last 30 layers trainable)\n",
      "   âœ… Stronger regularization (dropout 0.3-0.4)\n",
      "   âœ… HMM post-processing\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Fold 1/5 - Enhanced BiGRU_MultiHeadAttn\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.5557\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Computing CSV feature statistics for normalization...\n",
      "ðŸ“Š Found 16253 CSV features from 1084 videos\n",
      "âœ… CSV feature normalization fitted on 16253 samples\n",
      "\n",
      "ðŸš€ Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\n",
      "   âœ… Bidirectional GRU (256 + 128 units) with recurrent dropout\n",
      "   âœ… Dual Multi-Head Self-Attention (8 heads + 4 heads)\n",
      "   âœ… Layer Normalization after each major block\n",
      "   âœ… Enhanced data augmentation\n",
      "   âœ… Sequence length: 15 frames\n",
      "   âœ… MobileNet (last 30 layers trainable)\n",
      "   âœ… Stronger regularization (dropout 0.3-0.4)\n",
      "   âœ… HMM post-processing\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Fold 1/5 - Enhanced BiGRU_MultiHeadAttn\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.5557\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - ETA: 0s - loss: 0.8579 - accuracy: 0.5693\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Computing CSV feature statistics for normalization...\n",
      "ðŸ“Š Found 16253 CSV features from 1084 videos\n",
      "âœ… CSV feature normalization fitted on 16253 samples\n",
      "\n",
      "ðŸš€ Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\n",
      "   âœ… Bidirectional GRU (256 + 128 units) with recurrent dropout\n",
      "   âœ… Dual Multi-Head Self-Attention (8 heads + 4 heads)\n",
      "   âœ… Layer Normalization after each major block\n",
      "   âœ… Enhanced data augmentation\n",
      "   âœ… Sequence length: 15 frames\n",
      "   âœ… MobileNet (last 30 layers trainable)\n",
      "   âœ… Stronger regularization (dropout 0.3-0.4)\n",
      "   âœ… HMM post-processing\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Fold 1/5 - Enhanced BiGRU_MultiHeadAttn\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.5557\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - ETA: 0s - loss: 0.8579 - accuracy: 0.5693\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Computing CSV feature statistics for normalization...\n",
      "ðŸ“Š Found 16253 CSV features from 1084 videos\n",
      "âœ… CSV feature normalization fitted on 16253 samples\n",
      "\n",
      "ðŸš€ Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\n",
      "   âœ… Bidirectional GRU (256 + 128 units) with recurrent dropout\n",
      "   âœ… Dual Multi-Head Self-Attention (8 heads + 4 heads)\n",
      "   âœ… Layer Normalization after each major block\n",
      "   âœ… Enhanced data augmentation\n",
      "   âœ… Sequence length: 15 frames\n",
      "   âœ… MobileNet (last 30 layers trainable)\n",
      "   âœ… Stronger regularization (dropout 0.3-0.4)\n",
      "   âœ… HMM post-processing\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Fold 1/5 - Enhanced BiGRU_MultiHeadAttn\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.5557\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - ETA: 0s - loss: 0.8579 - accuracy: 0.5693\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - ETA: 0s - loss: 0.8280 - accuracy: 0.6209\n",
      "Epoch 3: val_accuracy improved from 0.55725 to 0.59542, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.55725 to 0.59542, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Computing CSV feature statistics for normalization...\n",
      "ðŸ“Š Found 16253 CSV features from 1084 videos\n",
      "âœ… CSV feature normalization fitted on 16253 samples\n",
      "\n",
      "ðŸš€ Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\n",
      "   âœ… Bidirectional GRU (256 + 128 units) with recurrent dropout\n",
      "   âœ… Dual Multi-Head Self-Attention (8 heads + 4 heads)\n",
      "   âœ… Layer Normalization after each major block\n",
      "   âœ… Enhanced data augmentation\n",
      "   âœ… Sequence length: 15 frames\n",
      "   âœ… MobileNet (last 30 layers trainable)\n",
      "   âœ… Stronger regularization (dropout 0.3-0.4)\n",
      "   âœ… HMM post-processing\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Fold 1/5 - Enhanced BiGRU_MultiHeadAttn\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.5557\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - ETA: 0s - loss: 0.8579 - accuracy: 0.5693\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - ETA: 0s - loss: 0.8280 - accuracy: 0.6209\n",
      "Epoch 3: val_accuracy improved from 0.55725 to 0.59542, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.55725 to 0.59542, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Computing CSV feature statistics for normalization...\n",
      "ðŸ“Š Found 16253 CSV features from 1084 videos\n",
      "âœ… CSV feature normalization fitted on 16253 samples\n",
      "\n",
      "ðŸš€ Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\n",
      "   âœ… Bidirectional GRU (256 + 128 units) with recurrent dropout\n",
      "   âœ… Dual Multi-Head Self-Attention (8 heads + 4 heads)\n",
      "   âœ… Layer Normalization after each major block\n",
      "   âœ… Enhanced data augmentation\n",
      "   âœ… Sequence length: 15 frames\n",
      "   âœ… MobileNet (last 30 layers trainable)\n",
      "   âœ… Stronger regularization (dropout 0.3-0.4)\n",
      "   âœ… HMM post-processing\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Fold 1/5 - Enhanced BiGRU_MultiHeadAttn\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "\n",
      "ðŸ“ˆ Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 15\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.5557\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.54198, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 145s 3s/step - loss: 0.9004 - accuracy: 0.5557 - val_loss: 0.7378 - val_accuracy: 0.5420 - lr: 1.0000e-04\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - ETA: 0s - loss: 0.8579 - accuracy: 0.5693\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.54198 to 0.55725, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "46/46 [==============================] - 130s 3s/step - loss: 0.8579 - accuracy: 0.5693 - val_loss: 0.7127 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - ETA: 0s - loss: 0.8280 - accuracy: 0.6209\n",
      "Epoch 3: val_accuracy improved from 0.55725 to 0.59542, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.55725 to 0.59542, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 266\u001b[0m\n\u001b[1;32m    263\u001b[0m earlystop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    264\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 266\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlystop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m all_histories\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m    270\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(model_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1850\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1847\u001b[0m     }\n\u001b[1;32m   1848\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1850\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1851\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks.py:453\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    451\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 453\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks.py:1483\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs_since_last_save \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1483\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks.py:1556\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_weights(\n\u001b[1;32m   1551\u001b[0m             filepath,\n\u001b[1;32m   1552\u001b[0m             overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1553\u001b[0m             options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options,\n\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1556\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079\u001b[0m, in \u001b[0;36mModel.save\u001b[0;34m(self, filepath, overwrite, save_format, **kwargs)\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Saves a model as a TensorFlow SavedModel or HDF5 file.\u001b[39;00m\n\u001b[1;32m   3028\u001b[0m \n\u001b[1;32m   3029\u001b[0m \u001b[38;5;124;03m    See the [Serialization and Saving guide](\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3077\u001b[0m \u001b[38;5;124;03m    Note that `model.save()` is an alias for `tf.keras.models.save_model()`.\u001b[39;00m\n\u001b[1;32m   3078\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3079\u001b[0m     \u001b[43msaving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/saving_api.py:167\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, save_format, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     saving_lib\u001b[38;5;241m.\u001b[39msave_model(model, filepath)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# Legacy case\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/legacy/save.py:160\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    150\u001b[0m         model, sequential\u001b[38;5;241m.\u001b[39mSequential\n\u001b[1;32m    151\u001b[0m     ):\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) or using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    159\u001b[0m         )\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model_to_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m serialization\u001b[38;5;241m.\u001b[39mSharedObjectSavingScope():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/legacy/hdf5_format.py:118\u001b[0m, in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    115\u001b[0m     opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     model_metadata \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model_metadata\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/legacy/saving_utils.py:168\u001b[0m, in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    166\u001b[0m model_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m require_config:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/functional.py:787\u001b[0m, in \u001b[0;36mFunctional.get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# Check whether the class has a constructor compatible with a Functional\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# model or if it has a custom constructor.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_functional_like_constructor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# Only return a Functional config if the constructor is the same\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# as that of a Functional model. This excludes subclassed Functional\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# models with a custom __init__.\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m     config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[43mget_network_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;66;03m# Try to autogenerate config\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     xtra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(config\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/functional.py:1593\u001b[0m, in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn, config)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Functional) \u001b[38;5;129;01mand\u001b[39;00m set_layers_legacy:\n\u001b[1;32m   1592\u001b[0m     layer\u001b[38;5;241m.\u001b[39muse_legacy_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1593\u001b[0m layer_config \u001b[38;5;241m=\u001b[39m \u001b[43mserialize_layer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1594\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1595\u001b[0m layer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minbound_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m filtered_inbound_nodes\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/legacy/serialization.py:304\u001b[0m, in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    302\u001b[0m name \u001b[38;5;241m=\u001b[39m object_registration\u001b[38;5;241m.\u001b[39mget_registered_name(instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _SKIP_FAILED_SERIALIZATION:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/rnn/base_wrapper.py:71\u001b[0m, in \u001b[0;36mWrapper.get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 71\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m         }\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# Case of incompatible custom wrappers\u001b[39;00m\n\u001b[1;32m     74\u001b[0m         config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m: legacy_serialization\u001b[38;5;241m.\u001b[39mserialize_keras_object(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer)\n\u001b[1;32m     76\u001b[0m         }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:240\u001b[0m, in \u001b[0;36mserialize_keras_object\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    229\u001b[0m         registered_name \u001b[38;5;241m=\u001b[39m object_registration\u001b[38;5;241m.\u001b[39mget_registered_name(\n\u001b[1;32m    230\u001b[0m             obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m    231\u001b[0m         )\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__typespec__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspec_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: spec_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistered_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: registered_name,\n\u001b[1;32m    238\u001b[0m     }\n\u001b[0;32m--> 240\u001b[0m inner_config \u001b[38;5;241m=\u001b[39m \u001b[43m_get_class_or_fn_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m config_with_public_class \u001b[38;5;241m=\u001b[39m serialize_with_public_class(\n\u001b[1;32m    242\u001b[0m     obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, inner_config\n\u001b[1;32m    243\u001b[0m )\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Add TF ops dispatch handler serialization for\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# ops.EagerTensor that contains nested numpy array.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Target: NetworkConstructionTest.test_constant_initializer_with_numpy\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:385\u001b[0m, in \u001b[0;36m_get_class_or_fn_config\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# All classes:\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_config\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 385\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `get_config()` method of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should return \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma dict. It returned: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/functional.py:787\u001b[0m, in \u001b[0;36mFunctional.get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# Check whether the class has a constructor compatible with a Functional\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# model or if it has a custom constructor.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_functional_like_constructor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# Only return a Functional config if the constructor is the same\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# as that of a Functional model. This excludes subclassed Functional\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# models with a custom __init__.\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m     config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[43mget_network_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;66;03m# Try to autogenerate config\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     xtra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(config\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/functional.py:1586\u001b[0m, in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn, config)\u001b[0m\n\u001b[1;32m   1582\u001b[0m     node_key \u001b[38;5;241m=\u001b[39m _make_node_key(layer\u001b[38;5;241m.\u001b[39mname, original_node_index)\n\u001b[1;32m   1583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node_key \u001b[38;5;129;01min\u001b[39;00m network\u001b[38;5;241m.\u001b[39m_network_nodes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node\u001b[38;5;241m.\u001b[39mis_input:\n\u001b[1;32m   1584\u001b[0m         \u001b[38;5;66;03m# The node is relevant to the model:\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;66;03m# add to filtered_inbound_nodes.\u001b[39;00m\n\u001b[0;32m-> 1586\u001b[0m         node_data \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_make_node_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_conversion_map\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m         filtered_inbound_nodes\u001b[38;5;241m.\u001b[39mappend(node_data)\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Functional) \u001b[38;5;129;01mand\u001b[39;00m set_layers_legacy:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/node.py:257\u001b[0m, in \u001b[0;36mNode.serialize\u001b[0;34m(self, make_node_key, node_conversion_map)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mis_nested(data)\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer\u001b[38;5;241m.\u001b[39m_preserve_input_structure_in_config\n\u001b[1;32m    255\u001b[0m ):\n\u001b[1;32m    256\u001b[0m     data \u001b[38;5;241m=\u001b[39m [data]\n\u001b[0;32m--> 257\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_inner_node_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:349\u001b[0m, in \u001b[0;36mconvert_inner_node_data\u001b[0;34m(nested, wrap)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m nested\u001b[38;5;241m.\u001b[39mas_list()\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nested\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_structure_with_atomic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_atomic_nested\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_convert_object_or_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:187\u001b[0m, in \u001b[0;36mmap_structure_with_atomic\u001b[0;34m(is_atomic_fn, map_fn, nested)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     values \u001b[38;5;241m=\u001b[39m nested\n\u001b[0;32m--> 187\u001b[0m mapped_values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    188\u001b[0m     map_structure_with_atomic(is_atomic_fn, map_fn, ele) \u001b[38;5;28;01mfor\u001b[39;00m ele \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[1;32m    189\u001b[0m ]\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39msequence_like(nested, mapped_values)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:187\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     values \u001b[38;5;241m=\u001b[39m nested\n\u001b[0;32m--> 187\u001b[0m mapped_values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    188\u001b[0m     map_structure_with_atomic(is_atomic_fn, map_fn, ele) \u001b[38;5;28;01mfor\u001b[39;00m ele \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[1;32m    189\u001b[0m ]\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39msequence_like(nested, mapped_values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, TimeDistributed, GRU, Dropout, Dense, GlobalAveragePooling2D, \n",
    "    Concatenate, BatchNormalization, Bidirectional, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "    )\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuration - Enhanced\n",
    "video_keys = list(video_dict.keys())\n",
    "video_labels = [labels[k] for k in video_keys]\n",
    "\n",
    "img_size = (224, 224)\n",
    "batch_size = 16  # Reduced for better generalization\n",
    "epochs = 100  # Increased for better convergence\n",
    "n_splits = 5\n",
    "sequence_len = 30  # Increased to capture more temporal information\n",
    "results = []\n",
    "all_histories = []\n",
    "\n",
    "# Normalize CSV features with robust scaling\n",
    "print(\"ðŸ”§ Computing CSV feature statistics for normalization...\")\n",
    "all_csv_features = []\n",
    "for key in video_keys:\n",
    "    frames = video_dict[key][:sequence_len]\n",
    "    for path in frames:\n",
    "        filename = os.path.basename(path)\n",
    "        if filename in openface_features:\n",
    "            all_csv_features.append(openface_features[filename])\n",
    "\n",
    "# Debug: Check if we have any features\n",
    "print(f\"ðŸ“Š Found {len(all_csv_features)} CSV features from {len(video_keys)} videos\")\n",
    "if len(all_csv_features) == 0:\n",
    "    print(\"âš ï¸ WARNING: No matching CSV features found!\")\n",
    "    print(f\"   Sample video filename: {os.path.basename(list(video_dict.values())[0][0])}\")\n",
    "    print(f\"   Sample CSV key: {list(openface_features.keys())[0]}\")\n",
    "    raise ValueError(\"No CSV features found - check filename matching!\")\n",
    "\n",
    "csv_scaler = StandardScaler()\n",
    "csv_scaler.fit(np.array(all_csv_features))  # Convert to numpy array\n",
    "print(f\"âœ… CSV feature normalization fitted on {len(all_csv_features)} samples\")\n",
    "\n",
    "# Enhanced Data generator with stronger augmentation\n",
    "class VideoSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, video_keys, video_dict, labels, batch_size, img_size, sequence_len=15, augment=False):\n",
    "        self.video_keys = video_keys\n",
    "        self.video_dict = video_dict\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.sequence_len = sequence_len\n",
    "        self.augment = augment\n",
    "        # Enhanced augmentation for better regularization\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20 if augment else 0,\n",
    "            width_shift_range=0.15 if augment else 0,\n",
    "            height_shift_range=0.15 if augment else 0,\n",
    "            zoom_range=0.15 if augment else 0,\n",
    "            horizontal_flip=augment,\n",
    "            brightness_range=[0.8, 1.2] if augment else None,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.video_keys) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_keys = self.video_keys[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_X_img, batch_X_csv, batch_y = [], [], []\n",
    "\n",
    "        for key in batch_keys:\n",
    "            frames = self.video_dict[key][:self.sequence_len]\n",
    "            imgs = []\n",
    "            csv_feats = []\n",
    "            \n",
    "            for path in frames:\n",
    "                # Load and process image\n",
    "                img = cv2.imread(path)\n",
    "                img = cv2.resize(img, self.img_size)\n",
    "                img = self.datagen.random_transform(img) if self.augment else img\n",
    "                img = img.astype('float32') / 255.0\n",
    "                imgs.append(img)\n",
    "                \n",
    "                # Load CSV features and normalize (1-to-1 mapping)\n",
    "                filename = os.path.basename(path)\n",
    "                if filename in openface_features:\n",
    "                    csv_feat = openface_features[filename]\n",
    "                    csv_feat = csv_scaler.transform(csv_feat.reshape(1, -1))[0]\n",
    "                else:\n",
    "                    csv_feat = np.zeros(csv_feature_dim, dtype='float32')\n",
    "                csv_feats.append(csv_feat)\n",
    "            \n",
    "            # Pad sequences if needed\n",
    "            while len(imgs) < self.sequence_len:\n",
    "                imgs.append(np.zeros((*self.img_size, 3), dtype='float32'))\n",
    "                csv_feats.append(np.zeros(csv_feature_dim, dtype='float32'))\n",
    "            \n",
    "            batch_X_img.append(imgs)\n",
    "            batch_X_csv.append(csv_feats)\n",
    "            batch_y.append(self.labels[key])\n",
    "\n",
    "        return [np.array(batch_X_img), np.array(batch_X_csv)], np.array(batch_y)\n",
    "\n",
    "# Enhanced BiGRU + Multi-Head Self-Attention Model\n",
    "def build_model(sequence_len, img_size, csv_dim=674):\n",
    "    # MobileNet branch with more trainable layers\n",
    "    base_cnn = MobileNetV2(input_shape=(*img_size, 3), include_top=False, weights='imagenet')\n",
    "    base_cnn.trainable = True\n",
    "    # Fine-tune more layers for better feature extraction\n",
    "    for layer in base_cnn.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    cnn_out = GlobalAveragePooling2D()(base_cnn.output)\n",
    "    cnn_model = Model(inputs=base_cnn.input, outputs=cnn_out)\n",
    "    \n",
    "    # Image sequence input\n",
    "    input_seq_img = Input(shape=(sequence_len, *img_size, 3), name='image_input')\n",
    "    x_img = TimeDistributed(cnn_model)(input_seq_img)\n",
    "    \n",
    "    # CSV features input\n",
    "    input_seq_csv = Input(shape=(sequence_len, csv_dim), name='csv_input')\n",
    "    \n",
    "    # BatchNormalization before fusion\n",
    "    x_img = BatchNormalization(name='bn_mobilenet')(x_img)\n",
    "    x_csv = BatchNormalization(name='bn_csv')(input_seq_csv)\n",
    "    \n",
    "    # Concatenate features (1-to-1 fusion)\n",
    "    x_combined = Concatenate(axis=-1, name='feature_fusion')([x_img, x_csv])\n",
    "    \n",
    "    # Enhanced BiGRU layers with more capacity\n",
    "    x = Bidirectional(GRU(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, name='bigru_1'))(x_combined)\n",
    "    x = LayerNormalization(name='ln_bigru_1')(x)\n",
    "    x = Dropout(0.3, name='dropout_1')(x)\n",
    "    \n",
    "    x = Bidirectional(GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, name='bigru_2'))(x)\n",
    "    x = LayerNormalization(name='ln_bigru_2')(x)\n",
    "    x = Dropout(0.3, name='dropout_2')(x)\n",
    "    \n",
    "    # Enhanced Multi-Head Self-Attention with more heads\n",
    "    x_norm = LayerNormalization(name='ln_before_attention')(x)\n",
    "    attn_output = MultiHeadAttention(\n",
    "        num_heads=8,  # Increased from 4 to 8 for better pattern capture\n",
    "        key_dim=64,   # Increased from 32 to 64\n",
    "        dropout=0.1,\n",
    "        name='multi_head_attention'\n",
    "    )(x_norm, x_norm)\n",
    "    \n",
    "    # Residual connection + Layer normalization\n",
    "    x = tf.keras.layers.Add(name='residual_connection')([x, attn_output])\n",
    "    x = LayerNormalization(name='ln_after_attention')(x)\n",
    "    x = Dropout(0.2, name='dropout_attention_1')(x)\n",
    "    \n",
    "    # Additional attention layer for deeper understanding\n",
    "    x_norm2 = LayerNormalization(name='ln_before_attention_2')(x)\n",
    "    attn_output2 = MultiHeadAttention(\n",
    "        num_heads=4,\n",
    "        key_dim=32,\n",
    "        dropout=0.1,\n",
    "        name='multi_head_attention_2'\n",
    "    )(x_norm2, x_norm2)\n",
    "    \n",
    "    x = tf.keras.layers.Add(name='residual_connection_2')([x, attn_output2])\n",
    "    x = LayerNormalization(name='ln_after_attention_2')(x)\n",
    "    \n",
    "    # Global average pooling over time dimension\n",
    "    x = GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "    x = Dropout(0.3, name='dropout_attention_2')(x)\n",
    "    \n",
    "    # Enhanced classification layers with more capacity\n",
    "    x = Dense(128, activation='relu', name='dense_1')(x)\n",
    "    x = BatchNormalization(name='bn_dense_1')(x)\n",
    "    x = Dropout(0.4, name='dropout_3')(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu', name='dense_2')(x)\n",
    "    x = BatchNormalization(name='bn_dense_2')(x)\n",
    "    x = Dropout(0.3, name='dropout_4')(x)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_seq_img, input_seq_csv], outputs=output, name='BiGRU_MultiHeadAttn')\n",
    "    return model\n",
    "\n",
    "# HMM post-processing (SAME NAME, unchanged)\n",
    "def hmm_postprocess(pred_probs, y_true, n_states=2):\n",
    "    pred_probs = pred_probs.reshape(-1, 1)\n",
    "    hmm = GaussianHMM(n_components=n_states, covariance_type=\"diag\", n_iter=100)\n",
    "    hmm.fit(pred_probs)\n",
    "    hidden_states = hmm.predict(pred_probs)\n",
    "\n",
    "    mapping = {}\n",
    "    for state in np.unique(hidden_states):\n",
    "        indices = [i for i in range(len(hidden_states)) if hidden_states[i] == state]\n",
    "        state_labels = [y_true[i] for i in indices]\n",
    "        if len(state_labels) > 0:\n",
    "            mapped_label = mode(state_labels, keepdims=True).mode[0]\n",
    "        else:\n",
    "            mapped_label = 0\n",
    "        mapping[state] = mapped_label\n",
    "\n",
    "    hmm_labels = np.array([mapping[s] for s in hidden_states])\n",
    "    return hmm_labels\n",
    "\n",
    "# Training K-Fold\n",
    "print(\"\\nðŸš€ Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\")\n",
    "print(\"   âœ… Bidirectional GRU (256 + 128 units) with recurrent dropout\")\n",
    "print(\"   âœ… Dual Multi-Head Self-Attention (8 heads + 4 heads)\")\n",
    "print(\"   âœ… Layer Normalization after each major block\")\n",
    "print(\"   âœ… Enhanced data augmentation\")\n",
    "print(\"   âœ… Sequence length: 15 frames\")\n",
    "print(\"   âœ… MobileNet (last 30 layers trainable)\")\n",
    "print(\"   âœ… Stronger regularization (dropout 0.3-0.4)\")\n",
    "print(\"   âœ… HMM post-processing\\n\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (trainval_idx, test_idx) in enumerate(skf.split(video_keys, video_labels), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“Š Fold {fold}/{n_splits} - Enhanced BiGRU_MultiHeadAttn\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    trainval_keys = [video_keys[i] for i in trainval_idx]\n",
    "    test_keys = [video_keys[i] for i in test_idx]\n",
    "\n",
    "    y_trainval = [labels[k] for k in trainval_keys]\n",
    "    train_keys, val_keys = train_test_split(trainval_keys, test_size=0.15, stratify=y_trainval, random_state=fold)\n",
    "\n",
    "    train_gen = VideoSequence(train_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=True)\n",
    "    val_gen = VideoSequence(val_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=False)\n",
    "    test_gen = VideoSequence(test_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=False)\n",
    "\n",
    "    model = build_model(sequence_len, img_size, csv_feature_dim)\n",
    "    \n",
    "    # Enhanced optimizer with learning rate warmup\n",
    "    initial_lr = 1e-4\n",
    "    model.compile(\n",
    "        optimizer=Adamax(learning_rate=initial_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Enhanced Model Architecture:\")\n",
    "    print(f\"   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\")\n",
    "    print(f\"   - Total params: {model.count_params():,}\")\n",
    "    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    print(f\"   - Trainable params: {trainable_params:,}\")\n",
    "    print(f\"   - Sequence length: {sequence_len}\")\n",
    "    print(f\"   - Batch size: {batch_size}\")\n",
    "    print(f\"   - Initial learning rate: {initial_lr}\")\n",
    "\n",
    "    model_path = f\"best_model_fold{fold}_bigru_mhattn_enhanced.h5\"\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
    "\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=epochs,\n",
    "                        callbacks=[checkpoint, earlystop, reduce_lr], verbose=1)\n",
    "    all_histories.append(history.history)\n",
    "\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    y_true = [labels[k] for k in test_keys]\n",
    "    y_pred_prob = model.predict(test_gen).ravel()\n",
    "    y_hmm_pred = hmm_postprocess(y_pred_prob, y_true)\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy_score(y_true, y_hmm_pred),\n",
    "        'precision': precision_score(y_true, y_hmm_pred),\n",
    "        'recall': recall_score(y_true, y_hmm_pred),\n",
    "        'f1': f1_score(y_true, y_hmm_pred),\n",
    "        'auc': roc_auc_score(y_true, y_pred_prob)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nâœ… Fold {fold} Results (Enhanced BiGRU_MultiHeadAttn):\")\n",
    "    print(f\"   Accuracy:  {results[-1]['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {results[-1]['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {results[-1]['recall']:.4f}\")\n",
    "    print(f\"   F1 Score:  {results[-1]['f1']:.4f}\")\n",
    "    print(f\"   AUC:       {results[-1]['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ðŸ“Š FINAL RESULTS - Enhanced BiGRU_MultiHeadAttn Model\")\n",
    "print(f\"{'='*60}\")\n",
    "for r in results:\n",
    "    print(f\"Fold {r['fold']}: Acc={r['accuracy']:.4f}, Prec={r['precision']:.4f}, Rec={r['recall']:.4f}, F1={r['f1']:.4f}, AUC={r['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Average Metrics:\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.mean(numeric_only=True))\n",
    "\n",
    "# Calculate CV metrics\n",
    "accuracy_mean = results_df['accuracy'].mean()\n",
    "accuracy_std = results_df['accuracy'].std()\n",
    "accuracy_range = results_df['accuracy'].max() - results_df['accuracy'].min()\n",
    "accuracy_cv_percent = (accuracy_std / accuracy_mean) * 100\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Cross-Validation Stability:\")\n",
    "print(f\"   Mean Accuracy: {accuracy_mean:.4f}\")\n",
    "print(f\"   Std Deviation: {accuracy_std:.4f}\")\n",
    "print(f\"   Range: {accuracy_range:.4f}\")\n",
    "print(f\"   CV%: {accuracy_cv_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfa766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T01:45:10.343234Z",
     "start_time": "2025-06-17T01:45:10.278158Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"ðŸ“Š Káº¿t quáº£ trung bÃ¬nh:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48802125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T07:31:56.035350Z",
     "start_time": "2025-06-14T07:31:50.121545Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, hist in enumerate(all_histories, 1):\n",
    "    plt.figure()\n",
    "    plt.plot(hist['accuracy'], label='Train Acc')\n",
    "    plt.plot(hist['val_accuracy'], label='Val Acc')\n",
    "    plt.title(f'Fold {i} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(hist['loss'], label='Train Loss')\n",
    "    plt.plot(hist['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Fold {i} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc4c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T07:32:05.949380Z",
     "start_time": "2025-06-14T07:32:05.516519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save mÃ´ hÃ¬nh fold cuá»‘i cÃ¹ng\n",
    "model.save(\"mobilenetv2_hmm_faceplus_final.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9581629f-92c8-4d93-88f5-8c57f3197a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"ðŸ“Š Káº¿t quáº£ trung bÃ¬nh:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167c3f6-2212-4cde-ab07-1ae1449219bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Giáº£ sá»­ results Ä‘Ã£ cÃ³ vÃ  báº¡n Ä‘Ã£ táº¡o results_df\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# TÃ­nh cÃ¡c chá»‰ sá»‘\n",
    "accuracy_mean = results_df['accuracy'].mean()\n",
    "accuracy_std = results_df['accuracy'].std()  # dÃ¹ng sample std (chia cho n-1)\n",
    "accuracy_range = results_df['accuracy'].max() - results_df['accuracy'].min()\n",
    "accuracy_cv_percent = (accuracy_std / accuracy_mean) * 100\n",
    "\n",
    "# In káº¿t quáº£\n",
    "print(\"ðŸ“Š Káº¿t quáº£ trung bÃ¬nh:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "\n",
    "print(f\"\\nâœ… CV Accuracy (Mean Accuracy): {accuracy_mean:.4f}\")\n",
    "print(f\"ðŸ“ˆ Range Accuracy: {accuracy_range:.4f}\")\n",
    "print(f\"ðŸ“‰ Accuracy CV% (std/mean): {accuracy_cv_percent:.2f}%\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ báº£ng káº¿t quáº£ náº¿u cáº§n\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4efdd-8ae0-4309-9f39-3a151116abb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
