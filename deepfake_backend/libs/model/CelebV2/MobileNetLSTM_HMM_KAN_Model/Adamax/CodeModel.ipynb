{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71f40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r /home/huuquangdang/huu.quang.dang/thesis/deepfake/deepfake_backend/libs/model/CelebV2/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149fd0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:44:38.582738Z",
     "start_time": "2025-08-13T09:44:38.568745Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# TensorFlow / Keras\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     Input, TimeDistributed, GlobalAveragePooling2D, LSTM, Dense, Dropout\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/__init__.py:42\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# from tensorflow.python.layers import layers\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m saved_model\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpu\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Sub-package for performing i/o directly instead of via ops in a graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/saved_model.py:20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Convenience functions to save a model.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m builder\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loader\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/builder.py:23\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"SavedModel builder.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mBuilds a SavedModel that can be saved to storage, is language neutral, and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03menables systems to produce, consume, or transform TensorFlow Models.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _SavedModelBuilder\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SavedModelBuilder\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# pylint: enable=unused-import\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/builder_impl.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m saved_model_pb2\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m saver_pb2\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_ml_dtypes\n\u001b[0;32m---> 37\u001b[0m _np_bfloat16 \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_ml_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m _np_float8_e4m3fn \u001b[38;5;241m=\u001b[39m pywrap_ml_dtypes\u001b[38;5;241m.\u001b[39mfloat8_e4m3fn()\n\u001b[1;32m     39\u001b[0m _np_float8_e5m2 \u001b[38;5;241m=\u001b[39m pywrap_ml_dtypes\u001b[38;5;241m.\u001b[39mfloat8_e5m2()\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "# Built-in\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, TimeDistributed, GlobalAveragePooling2D, LSTM, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecf6429cdc7d99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:06:51.462172Z",
     "start_time": "2025-06-16T15:06:51.281795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "base_path = '/home/huuquangdang/huu.quang.dang/thesis/Dataset/celeb_df_crop'\n",
    "categories = ['fake', 'real']\n",
    "\n",
    "# Initialize a list to hold data\n",
    "data = []\n",
    "\n",
    "# Process each category\n",
    "for category in categories:\n",
    "    category_path = os.path.join(base_path, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            try:\n",
    "                id_part, frame_part = filename.split('_frame_')\n",
    "                id_ = id_part.split('_')[0]\n",
    "                frame = frame_part.split('.')[0]\n",
    "                data.append({\n",
    "                    'filename': filename,\n",
    "                    'path': os.path.join(category_path, filename),\n",
    "                    'id': int(id_),\n",
    "                    'frame': int(frame),\n",
    "                    'label': category\n",
    "                })\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17e5b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:06:53.591689Z",
     "start_time": "2025-06-16T15:06:53.575495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>frame</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264_id37_0009_frame_0001.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>271_id38_0006_frame_0014.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>271</td>\n",
       "      <td>14</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>015_id10_0006_frame_0027.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>392_id4_0007_frame_0028.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>392</td>\n",
       "      <td>28</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029_id11_0010_frame_0004.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32367</th>\n",
       "      <td>475_id58_0004_frame_0012.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>475</td>\n",
       "      <td>12</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32368</th>\n",
       "      <td>014_id10_0005_frame_0012.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32369</th>\n",
       "      <td>365_id48_0000_frame_0023.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>365</td>\n",
       "      <td>23</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32370</th>\n",
       "      <td>091_id20_0000_frame_0026.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>91</td>\n",
       "      <td>26</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32371</th>\n",
       "      <td>356_id47_0001_frame_0019.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>356</td>\n",
       "      <td>19</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32372 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0      264_id37_0009_frame_0001.jpg   \n",
       "1      271_id38_0006_frame_0014.jpg   \n",
       "2      015_id10_0006_frame_0027.jpg   \n",
       "3       392_id4_0007_frame_0028.jpg   \n",
       "4      029_id11_0010_frame_0004.jpg   \n",
       "...                             ...   \n",
       "32367  475_id58_0004_frame_0012.jpg   \n",
       "32368  014_id10_0005_frame_0012.jpg   \n",
       "32369  365_id48_0000_frame_0023.jpg   \n",
       "32370  091_id20_0000_frame_0026.jpg   \n",
       "32371  356_id47_0001_frame_0019.jpg   \n",
       "\n",
       "                                                    path   id  frame label  \\\n",
       "0      /home/huuquangdang/huu.quang.dang/thesis/Datas...  264      1  fake   \n",
       "1      /home/huuquangdang/huu.quang.dang/thesis/Datas...  271     14  fake   \n",
       "2      /home/huuquangdang/huu.quang.dang/thesis/Datas...   15     27  fake   \n",
       "3      /home/huuquangdang/huu.quang.dang/thesis/Datas...  392     28  fake   \n",
       "4      /home/huuquangdang/huu.quang.dang/thesis/Datas...   29      4  fake   \n",
       "...                                                  ...  ...    ...   ...   \n",
       "32367  /home/huuquangdang/huu.quang.dang/thesis/Datas...  475     12  real   \n",
       "32368  /home/huuquangdang/huu.quang.dang/thesis/Datas...   14     12  real   \n",
       "32369  /home/huuquangdang/huu.quang.dang/thesis/Datas...  365     23  real   \n",
       "32370  /home/huuquangdang/huu.quang.dang/thesis/Datas...   91     26  real   \n",
       "32371  /home/huuquangdang/huu.quang.dang/thesis/Datas...  356     19  real   \n",
       "\n",
       "       label_id  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "32367         1  \n",
       "32368         1  \n",
       "32369         1  \n",
       "32370         1  \n",
       "32371         1  \n",
       "\n",
       "[32372 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ƒê·∫£m b·∫£o ƒë√£ c√≥ df_cropped.csv ch·ª©a ƒë∆∞·ªùng d·∫´n ·∫£nh ƒë√£ crop\n",
    "df['label_id'] = df['label'].map({'fake': 0, 'real': 1})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e6874-6fe8-4dff-9f0d-5c7d94337ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:06:56.671685Z",
     "start_time": "2025-06-16T15:06:55.874053Z"
    }
   },
   "outputs": [],
   "source": [
    "df['video_key'] = df['id'].astype(str) + \"_\" + df['label']\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "video_dict = defaultdict(list)\n",
    "labels = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    key = row['video_key']\n",
    "    video_dict[key].append(row['path'])\n",
    "    labels[key] = row['label_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8891d0-4a1f-4bcd-97f2-0b9577706cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T06:50:19.887250Z",
     "start_time": "2025-06-16T06:50:19.279020Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b99374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from tfkan.layers import Conv2DKAN, DenseKAN  # Import KAN layers\n",
    "from scipy.stats import mode\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "\n",
    "# Gi·∫£ ƒë·ªãnh ƒë√£ c√≥\n",
    "video_keys = list(video_dict.keys())\n",
    "video_labels = [labels[k] for k in video_keys]\n",
    "\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "n_splits = 5\n",
    "sequence_len = 10\n",
    "results = []\n",
    "all_histories = []\n",
    "\n",
    "# Data generator\n",
    "class VideoSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, video_keys, video_dict, labels, batch_size, img_size, sequence_len=10, augment=False):\n",
    "        self.video_keys = video_keys\n",
    "        self.video_dict = video_dict\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.sequence_len = sequence_len\n",
    "        self.augment = augment\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=15 if augment else 0,\n",
    "            zoom_range=0.1 if augment else 0,\n",
    "            horizontal_flip=augment\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.video_keys) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_keys = self.video_keys[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_X, batch_y = [], []\n",
    "\n",
    "        for key in batch_keys:\n",
    "            frames = self.video_dict[key][:self.sequence_len]\n",
    "            imgs = []\n",
    "            for path in frames:\n",
    "                img = cv2.imread(path)\n",
    "                img = cv2.resize(img, self.img_size)\n",
    "                img = self.datagen.random_transform(img) if self.augment else img\n",
    "                img = img.astype('float32') / 255.0\n",
    "                imgs.append(img)\n",
    "            while len(imgs) < self.sequence_len:\n",
    "                imgs.append(np.zeros((*self.img_size, 3), dtype='float32'))\n",
    "            batch_X.append(imgs)\n",
    "            batch_y.append(self.labels[key])\n",
    "\n",
    "        return np.array(batch_X), np.array(batch_y)\n",
    "\n",
    "# Build model c·∫£i ti·∫øn\n",
    "def build_model(sequence_len, img_size):\n",
    "    base_cnn = MobileNetV2(input_shape=(*img_size, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "    # Freeze to√†n b·ªô backbone ƒë·ªÉ gi·∫£m overfit\n",
    "    base_cnn.trainable = False\n",
    "\n",
    "    # CNN feature extractor\n",
    "    cnn_out = GlobalAveragePooling2D()(base_cnn.output)\n",
    "    cnn_model = Model(inputs=base_cnn.input, outputs=cnn_out)\n",
    "\n",
    "    # Sequence input\n",
    "    input_seq = Input(shape=(sequence_len, *img_size, 3))\n",
    "    x = TimeDistributed(cnn_model)(input_seq)\n",
    "\n",
    "    # Temporal modeling\n",
    "    x = LSTM(64, return_sequences=False)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Classification\n",
    "    x = DenseKAN(1)(x)\n",
    "    output = tf.keras.activations.sigmoid(x)\n",
    "\n",
    "    model = Model(inputs=input_seq, outputs=output)\n",
    "    return model\n",
    "\n",
    "# HMM c·∫£i ti·∫øn\n",
    "def hmm_postprocess(pred_probs, y_true, n_states=2):\n",
    "    pred_probs = pred_probs.reshape(-1, 1)\n",
    "    hmm = GaussianHMM(n_components=n_states, covariance_type=\"diag\", n_iter=100)\n",
    "    hmm.fit(pred_probs)\n",
    "    hidden_states = hmm.predict(pred_probs)\n",
    "\n",
    "    mapping = {}\n",
    "    for state in np.unique(hidden_states):\n",
    "        indices = [i for i in range(len(hidden_states)) if hidden_states[i] == state]\n",
    "        state_labels = [y_true[i] for i in indices]\n",
    "        if len(state_labels) > 0:\n",
    "            mapped_label = mode(state_labels, keepdims=True).mode[0]\n",
    "        else:\n",
    "            mapped_label = 0  # fallback\n",
    "        mapping[state] = mapped_label\n",
    "\n",
    "    hmm_labels = np.array([mapping[s] for s in hidden_states])\n",
    "    return hmm_labels\n",
    "\n",
    "# Training K-Fold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (trainval_idx, test_idx) in enumerate(skf.split(video_keys, video_labels), 1):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    trainval_keys = [video_keys[i] for i in trainval_idx]\n",
    "    test_keys = [video_keys[i] for i in test_idx]\n",
    "\n",
    "    y_trainval = [labels[k] for k in trainval_keys]\n",
    "    train_keys, val_keys = train_test_split(trainval_keys, test_size=0.1, stratify=y_trainval, random_state=fold)\n",
    "\n",
    "    train_gen = VideoSequence(train_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=True)\n",
    "    val_gen = VideoSequence(val_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=False)\n",
    "    test_gen = VideoSequence(test_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=False)\n",
    "\n",
    "    model = build_model(sequence_len, img_size)\n",
    "    model.compile(optimizer=Adamax(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model_path = f\"best_model_fold{fold}.h5\"\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=epochs,\n",
    "                        callbacks=[checkpoint, earlystop, reduce_lr], verbose=1)\n",
    "    all_histories.append(history.history)\n",
    "\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    y_true = [labels[k] for k in test_keys]\n",
    "    y_pred_prob = model.predict(test_gen).ravel()\n",
    "    y_hmm_pred = hmm_postprocess(y_pred_prob, y_true)\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy_score(y_true, y_hmm_pred),\n",
    "        'precision': precision_score(y_true, y_hmm_pred),\n",
    "        'recall': recall_score(y_true, y_hmm_pred),\n",
    "        'f1': f1_score(y_true, y_hmm_pred),\n",
    "        'auc': roc_auc_score(y_true, y_pred_prob)\n",
    "    })\n",
    "\n",
    "print(\"\\nüìä T·ªïng k·∫øt k·∫øt qu·∫£ c√°c fold:\")\n",
    "for r in results:\n",
    "    print(f\"Fold {r['fold']}: Accuracy={r['accuracy']:.4f}, F1={r['f1']:.4f}, AUC={r['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96c251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T20:36:15.411930Z",
     "start_time": "2025-06-16T16:43:06.606942Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from tfkan.layers import Conv2DKAN, DenseKAN  # Import KAN layers\n",
    "from scipy.stats import mode\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "\n",
    "# Gi·∫£ ƒë·ªãnh ƒë√£ c√≥\n",
    "video_keys = list(video_dict.keys())\n",
    "video_labels = [labels[k] for k in video_keys]\n",
    "\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "n_splits = 5\n",
    "sequence_len = 10\n",
    "results = []\n",
    "all_histories = []\n",
    "\n",
    "# Data generator\n",
    "class VideoSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, video_keys, video_dict, labels, batch_size, img_size, sequence_len=10, augment=False):\n",
    "        self.video_keys = video_keys\n",
    "        self.video_dict = video_dict\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.sequence_len = sequence_len\n",
    "        self.augment = augment\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=15 if augment else 0,\n",
    "            zoom_range=0.1 if augment else 0,\n",
    "            horizontal_flip=augment\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.video_keys) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_keys = self.video_keys[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_X, batch_y = [], []\n",
    "\n",
    "        for key in batch_keys:\n",
    "            frames = self.video_dict[key][:self.sequence_len]\n",
    "            imgs = []\n",
    "            for path in frames:\n",
    "                img = cv2.imread(path)\n",
    "                img = cv2.resize(img, self.img_size)\n",
    "                img = self.datagen.random_transform(img) if self.augment else img\n",
    "                img = img.astype('float32') / 255.0\n",
    "                imgs.append(img)\n",
    "            while len(imgs) < self.sequence_len:\n",
    "                imgs.append(np.zeros((*self.img_size, 3), dtype='float32'))\n",
    "            batch_X.append(imgs)\n",
    "            batch_y.append(self.labels[key])\n",
    "\n",
    "        return np.array(batch_X), np.array(batch_y)\n",
    "\n",
    "# Build model c·∫£i ti·∫øn\n",
    "def build_model(sequence_len, img_size):\n",
    "    base_cnn = MobileNetV2(input_shape=(*img_size, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "    # Freeze to√†n b·ªô backbone ƒë·ªÉ gi·∫£m overfit\n",
    "    base_cnn.trainable = False\n",
    "\n",
    "    # CNN feature extractor\n",
    "    cnn_out = GlobalAveragePooling2D()(base_cnn.output)\n",
    "    cnn_model = Model(inputs=base_cnn.input, outputs=cnn_out)\n",
    "\n",
    "    # Sequence input\n",
    "    input_seq = Input(shape=(sequence_len, *img_size, 3))\n",
    "    x = TimeDistributed(cnn_model)(input_seq)\n",
    "\n",
    "    # Temporal modeling\n",
    "    x = LSTM(64, return_sequences=False)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Classification\n",
    "    x = DenseKAN(1)(x)\n",
    "    output = tf.keras.activations.sigmoid(x)\n",
    "\n",
    "    model = Model(inputs=input_seq, outputs=output)\n",
    "    return model\n",
    "\n",
    "# HMM c·∫£i ti·∫øn\n",
    "def hmm_postprocess(pred_probs, y_true, n_states=2):\n",
    "    pred_probs = pred_probs.reshape(-1, 1)\n",
    "    hmm = GaussianHMM(n_components=n_states, covariance_type=\"diag\", n_iter=100)\n",
    "    hmm.fit(pred_probs)\n",
    "    hidden_states = hmm.predict(pred_probs)\n",
    "\n",
    "    mapping = {}\n",
    "    for state in np.unique(hidden_states):\n",
    "        indices = [i for i in range(len(hidden_states)) if hidden_states[i] == state]\n",
    "        state_labels = [y_true[i] for i in indices]\n",
    "        if len(state_labels) > 0:\n",
    "            mapped_label = mode(state_labels, keepdims=True).mode[0]\n",
    "        else:\n",
    "            mapped_label = 0  # fallbac\n",
    "            # k\n",
    "        mapping[state] = mapped_label\n",
    "\n",
    "    hmm_labels = np.array([mapping[s] for s in hidden_states])\n",
    "    return hmm_labels\n",
    "\n",
    "# Training K-Fold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (trainval_idx, test_idx) in enumerate(skf.split(video_keys, video_labels), 1):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    trainval_keys = [video_keys[i] for i in trainval_idx]\n",
    "    test_keys = [video_keys[i] for i in test_idx]\n",
    "\n",
    "    y_trainval = [labels[k] for k in trainval_keys]\n",
    "    train_keys, val_keys = train_test_split(trainval_keys, test_size=0.1, stratify=y_trainval, random_state=fold)\n",
    "\n",
    "    train_gen = VideoSequence(train_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=True)\n",
    "    val_gen = VideoSequence(val_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=False)\n",
    "    test_gen = VideoSequence(test_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=False)\n",
    "\n",
    "    model = build_model(sequence_len, img_size)\n",
    "    model.compile(optimizer=Adamax(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model_path = f\"best_model_fold{fold}.h5\"\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=epochs,\n",
    "                        callbacks=[checkpoint, earlystop, reduce_lr], verbose=1)\n",
    "    all_histories.append(history.history)\n",
    "\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    y_true = [labels[k] for k in test_keys]\n",
    "    y_pred_prob = model.predict(test_gen).ravel()\n",
    "    y_hmm_pred = hmm_postprocess(y_pred_prob, y_true)\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy_score(y_true, y_hmm_pred),\n",
    "        'precision': precision_score(y_true, y_hmm_pred),\n",
    "        'recall': recall_score(y_true, y_hmm_pred),\n",
    "        'f1': f1_score(y_true, y_hmm_pred),\n",
    "        'auc': roc_auc_score(y_true, y_pred_prob)\n",
    "    })\n",
    "\n",
    "print(\"\\nüìä T·ªïng k·∫øt k·∫øt qu·∫£ c√°c fold:\")\n",
    "for r in results:\n",
    "    print(f\"Fold {r['fold']}: Accuracy={r['accuracy']:.4f}, F1={r['f1']:.4f}, AUC={r['auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfa766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T01:45:10.343234Z",
     "start_time": "2025-06-17T01:45:10.278158Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"üìä K·∫øt qu·∫£ trung b√¨nh:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48802125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T07:31:56.035350Z",
     "start_time": "2025-06-14T07:31:50.121545Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, hist in enumerate(all_histories, 1):\n",
    "    plt.figure()\n",
    "    plt.plot(hist['accuracy'], label='Train Acc')\n",
    "    plt.plot(hist['val_accuracy'], label='Val Acc')\n",
    "    plt.title(f'Fold {i} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(hist['loss'], label='Train Loss')\n",
    "    plt.plot(hist['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Fold {i} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc4c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T07:32:05.949380Z",
     "start_time": "2025-06-14T07:32:05.516519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save m√¥ h√¨nh fold cu·ªëi c√πng\n",
    "model.save(\"mobilenetv2_hmm_faceplus_final.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9581629f-92c8-4d93-88f5-8c57f3197a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"üìä K·∫øt qu·∫£ trung b√¨nh:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167c3f6-2212-4cde-ab07-1ae1449219bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Gi·∫£ s·ª≠ results ƒë√£ c√≥ v√† b·∫°n ƒë√£ t·∫°o results_df\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# T√≠nh c√°c ch·ªâ s·ªë\n",
    "accuracy_mean = results_df['accuracy'].mean()\n",
    "accuracy_std = results_df['accuracy'].std()  # d√πng sample std (chia cho n-1)\n",
    "accuracy_range = results_df['accuracy'].max() - results_df['accuracy'].min()\n",
    "accuracy_cv_percent = (accuracy_std / accuracy_mean) * 100\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(\"üìä K·∫øt qu·∫£ trung b√¨nh:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "\n",
    "print(f\"\\n‚úÖ CV Accuracy (Mean Accuracy): {accuracy_mean:.4f}\")\n",
    "print(f\"üìà Range Accuracy: {accuracy_range:.4f}\")\n",
    "print(f\"üìâ Accuracy CV% (std/mean): {accuracy_cv_percent:.2f}%\")\n",
    "\n",
    "# Hi·ªÉn th·ªã b·∫£ng k·∫øt qu·∫£ n·∫øu c·∫ßn\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4efdd-8ae0-4309-9f39-3a151116abb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
