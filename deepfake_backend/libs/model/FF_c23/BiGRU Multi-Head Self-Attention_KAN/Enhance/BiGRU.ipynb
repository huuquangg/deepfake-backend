{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb6a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r /home/huuquangdang/huu.quang.dang/thesis/deepfake/deepfake_backend/libs/model/CelebV2/requirements.txt\n",
    "\n",
    "# # Step 1: Uninstall incompatible NumPy version\n",
    "# %pip uninstall numpy -y\n",
    "# # Step 2: Install compatible NumPy version (< 2.0.0)\n",
    "# %pip install \"numpy>=1.21.0,<2.0.0\"\n",
    "# # Step 3: Verify NumPy version\n",
    "# import numpy as np\n",
    "# print(f\"✅ NumPy version: {np.__version__}\")\n",
    "# print(f\"Expected: < 2.0.0 (you should see 1.x.x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149fd0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:44:38.582738Z",
     "start_time": "2025-08-13T09:44:38.568745Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 21:43:34.413447: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-27 21:43:34.466144: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-27 21:43:34.791770: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-27 21:43:34.791834: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-27 21:43:34.793456: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-27 21:43:35.029085: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-27 21:43:35.031431: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-27 21:43:35.961367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Built-in\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, TimeDistributed, GlobalAveragePooling2D, LSTM, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ecf6429cdc7d99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:06:51.462172Z",
     "start_time": "2025-06-16T15:06:51.281795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "base_path = '/home/huuquangdang/huu.quang.dang/thesis/Dataset/facep/faceplus_processed_research_final_2_crop'\n",
    "categories = ['fake', 'real']\n",
    "\n",
    "# Initialize a list to hold data\n",
    "data = []\n",
    "\n",
    "# Process each category\n",
    "for category in categories:\n",
    "    category_path = os.path.join(base_path, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            try:\n",
    "                id_part, frame_part = filename.split('_frame_')\n",
    "                id_ = id_part.split('_')[0]\n",
    "                frame = frame_part.split('.')[0]\n",
    "                data.append({\n",
    "                    'filename': filename,\n",
    "                    'path': os.path.join(category_path, filename),\n",
    "                    'id': int(id_),\n",
    "                    'frame': int(frame),\n",
    "                    'label': category\n",
    "                })\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc17e5b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:06:53.591689Z",
     "start_time": "2025-06-16T15:06:53.575495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>frame</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>855_801_frame_0018.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>855</td>\n",
       "      <td>18</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>686_696_frame_0024.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>686</td>\n",
       "      <td>24</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664_668_frame_0017.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>664</td>\n",
       "      <td>17</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>486_680_frame_0010.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>486</td>\n",
       "      <td>10</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>464_463_frame_0005.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>464</td>\n",
       "      <td>5</td>\n",
       "      <td>fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59803</th>\n",
       "      <td>140_frame_0025.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>140</td>\n",
       "      <td>25</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59804</th>\n",
       "      <td>937_frame_0007.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>937</td>\n",
       "      <td>7</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59805</th>\n",
       "      <td>092_frame_0001.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59806</th>\n",
       "      <td>736_frame_0018.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>736</td>\n",
       "      <td>18</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59807</th>\n",
       "      <td>958_frame_0023.jpg</td>\n",
       "      <td>/home/huuquangdang/huu.quang.dang/thesis/Datas...</td>\n",
       "      <td>958</td>\n",
       "      <td>23</td>\n",
       "      <td>real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59808 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  \\\n",
       "0      855_801_frame_0018.jpg   \n",
       "1      686_696_frame_0024.jpg   \n",
       "2      664_668_frame_0017.jpg   \n",
       "3      486_680_frame_0010.jpg   \n",
       "4      464_463_frame_0005.jpg   \n",
       "...                       ...   \n",
       "59803      140_frame_0025.jpg   \n",
       "59804      937_frame_0007.jpg   \n",
       "59805      092_frame_0001.jpg   \n",
       "59806      736_frame_0018.jpg   \n",
       "59807      958_frame_0023.jpg   \n",
       "\n",
       "                                                    path   id  frame label  \\\n",
       "0      /home/huuquangdang/huu.quang.dang/thesis/Datas...  855     18  fake   \n",
       "1      /home/huuquangdang/huu.quang.dang/thesis/Datas...  686     24  fake   \n",
       "2      /home/huuquangdang/huu.quang.dang/thesis/Datas...  664     17  fake   \n",
       "3      /home/huuquangdang/huu.quang.dang/thesis/Datas...  486     10  fake   \n",
       "4      /home/huuquangdang/huu.quang.dang/thesis/Datas...  464      5  fake   \n",
       "...                                                  ...  ...    ...   ...   \n",
       "59803  /home/huuquangdang/huu.quang.dang/thesis/Datas...  140     25  real   \n",
       "59804  /home/huuquangdang/huu.quang.dang/thesis/Datas...  937      7  real   \n",
       "59805  /home/huuquangdang/huu.quang.dang/thesis/Datas...   92      1  real   \n",
       "59806  /home/huuquangdang/huu.quang.dang/thesis/Datas...  736     18  real   \n",
       "59807  /home/huuquangdang/huu.quang.dang/thesis/Datas...  958     23  real   \n",
       "\n",
       "       label_id  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "59803         1  \n",
       "59804         1  \n",
       "59805         1  \n",
       "59806         1  \n",
       "59807         1  \n",
       "\n",
       "[59808 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Đảm bảo đã có df_cropped.csv chứa đường dẫn ảnh đã crop\n",
    "df['label_id'] = df['label'].map({'fake': 0, 'real': 1})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c24e6874-6fe8-4dff-9f0d-5c7d94337ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:06:56.671685Z",
     "start_time": "2025-06-16T15:06:55.874053Z"
    }
   },
   "outputs": [],
   "source": [
    "df['video_key'] = df['id'].astype(str) + \"_\" + df['label']\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "video_dict = defaultdict(list)\n",
    "labels = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    key = row['video_key']\n",
    "    video_dict[key].append(row['path'])\n",
    "    labels[key] = row['label_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8891d0-4a1f-4bcd-97f2-0b9577706cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T06:50:19.887250Z",
     "start_time": "2025-06-16T06:50:19.279020Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016d786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading pre-extracted OpenFace features from CSV files...\n",
      "✅ Loaded 59808 feature vectors\n",
      "✅ Each vector has 674 dimensions\n",
      "✅ Feature dictionary ready for 1-to-1 frame mapping\n"
     ]
    }
   ],
   "source": [
    "# Load pre-extracted CSV features for feature fusion\n",
    "print(\"📂 Loading pre-extracted OpenFace features from CSV files...\")\n",
    "csv_fake_path = '/home/huuquangdang/huu.quang.dang/thesis/deepfake/deepfake_backend/libs/tools/fake/fpp_fake_v1.csv'\n",
    "csv_real_path = '/home/huuquangdang/huu.quang.dang/thesis/deepfake/deepfake_backend/libs/tools/real/fpp_real_v1.csv'\n",
    "\n",
    "df_fake_features = pd.read_csv(csv_fake_path, header=None)\n",
    "df_real_features = pd.read_csv(csv_real_path, header=None)\n",
    "\n",
    "# Combine both dataframes\n",
    "df_all_features = pd.concat([df_fake_features, df_real_features], ignore_index=True)\n",
    "df_all_features.columns = ['filename'] + [f'feat_{i}' for i in range(df_all_features.shape[1] - 1)]\n",
    "\n",
    "# Create a dictionary for fast lookup: filename -> features\n",
    "openface_features = {}\n",
    "for _, row in df_all_features.iterrows():\n",
    "    filename = row['filename']\n",
    "    features = row.iloc[1:].values.astype('float32')\n",
    "    openface_features[filename] = features\n",
    "\n",
    "csv_feature_dim = len(features)\n",
    "print(f\"✅ Loaded {len(openface_features)} feature vectors\")\n",
    "print(f\"✅ Each vector has {csv_feature_dim} dimensions\")\n",
    "print(f\"✅ Feature dictionary ready for 1-to-1 frame mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b762d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Debugging filename matching:\n",
      "\n",
      "📁 Sample image filenames (from video_dict):\n",
      "   - 855_801_frame_0018.jpg\n",
      "   - 855_801_frame_0011.jpg\n",
      "   - 855_801_frame_0016.jpg\n",
      "\n",
      "📄 Sample CSV feature keys (from openface_features):\n",
      "   - 000_003_frame_0000.jpg\n",
      "   - 000_003_frame_0001.jpg\n",
      "   - 000_003_frame_0002.jpg\n",
      "   - 000_003_frame_0003.jpg\n",
      "   - 000_003_frame_0004.jpg\n",
      "\n",
      "🔎 Checking for matches:\n",
      "   ✅ MATCH: 855_801_frame_0018.jpg\n",
      "   ✅ MATCH: 855_801_frame_0011.jpg\n",
      "   ✅ MATCH: 855_801_frame_0016.jpg\n",
      "\n",
      "📊 Total matches: 3/3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 🔍 Debug: Check filename matching between images and CSV features\n",
    "print(\"🔍 Debugging filename matching:\")\n",
    "print(f\"\\n📁 Sample image filenames (from video_dict):\")\n",
    "sample_paths = list(video_dict.values())[0][:3]\n",
    "for path in sample_paths:\n",
    "    print(f\"   - {os.path.basename(path)}\")\n",
    "\n",
    "print(f\"\\n📄 Sample CSV feature keys (from openface_features):\")\n",
    "sample_csv_keys = list(openface_features.keys())[:5]\n",
    "for key in sample_csv_keys:\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "# Check for matches\n",
    "print(f\"\\n🔎 Checking for matches:\")\n",
    "matches = 0\n",
    "for path in sample_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    if filename in openface_features:\n",
    "        print(f\"   ✅ MATCH: {filename}\")\n",
    "        matches += 1\n",
    "    else:\n",
    "        print(f\"   ❌ NO MATCH: {filename}\")\n",
    "        # Try to find similar names\n",
    "        similar = [k for k in list(openface_features.keys())[:10] if filename[:10] in k or k[:10] in filename]\n",
    "        if similar:\n",
    "            print(f\"      Similar: {similar[:3]}\")\n",
    "\n",
    "print(f\"\\n📊 Total matches: {matches}/{len(sample_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfba17a",
   "metadata": {},
   "source": [
    "# 🔬 Model: BiGRU + Multi-Head Self-Attention\n",
    "\n",
    "This cell implements **BiGRU with Multi-Head Self-Attention mechanism** for improved sequence modeling, while maintaining:\n",
    "- ✅ Same function names (`VideoSequence`, `build_model`, `hmm_postprocess`)\n",
    "- ✅ Same 1-to-1 feature fusion (MobileNet + CSV)\n",
    "- ✅ Same normalization strategy\n",
    "- ✅ Enhanced with Bidirectional GRU and Multi-Head Self-Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Computing CSV feature statistics for normalization...\n",
      "📊 Found 59808 CSV features from 2000 videos\n",
      "✅ CSV feature normalization fitted on 59808 samples\n",
      "\n",
      "🚀 Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\n",
      "   ✅ Bidirectional GRU (256 + 128 units) with recurrent dropout\n",
      "   ✅ Dual Multi-Head Self-Attention (8 heads + 4 heads)\n",
      "   ✅ Layer Normalization after each major block\n",
      "   ✅ Enhanced data augmentation\n",
      "   ✅ Sequence length: 15 frames\n",
      "   ✅ MobileNet (last 30 layers trainable)\n",
      "   ✅ Stronger regularization (dropout 0.3-0.4)\n",
      "   ✅ HMM post-processing\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 Fold 1/5 - Enhanced BiGRU_MultiHeadAttn\n",
      "============================================================\n",
      "\n",
      "📈 Enhanced Model Architecture:\n",
      "   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\n",
      "   - Total params: 6,859,849\n",
      "   - Trainable params: 6,123,973\n",
      "   - Sequence length: 30\n",
      "   - Batch size: 16\n",
      "   - Initial learning rate: 0.0001\n",
      "Epoch 1/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.8410 - accuracy: 0.5809\n",
      "Epoch 1: val_accuracy improved from -inf to 0.60833, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 524s 6s/step - loss: 0.8410 - accuracy: 0.5809 - val_loss: 0.7738 - val_accuracy: 0.6083 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.6140 - accuracy: 0.6904\n",
      "Epoch 2: val_accuracy improved from 0.60833 to 0.62917, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "85/85 [==============================] - 503s 6s/step - loss: 0.6140 - accuracy: 0.6904 - val_loss: 0.8694 - val_accuracy: 0.6292 - lr: 1.0000e-04\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.7581\n",
      "Epoch 3: val_accuracy improved from 0.62917 to 0.68750, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "85/85 [==============================] - 500s 6s/step - loss: 0.5261 - accuracy: 0.7581 - val_loss: 1.0481 - val_accuracy: 0.6875 - lr: 1.0000e-04\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - ETA: 0s - loss: 0.4926 - accuracy: 0.7581\n",
      "Epoch 4: val_accuracy improved from 0.68750 to 0.73750, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "85/85 [==============================] - 505s 6s/step - loss: 0.4926 - accuracy: 0.7581 - val_loss: 0.8247 - val_accuracy: 0.7375 - lr: 1.0000e-04\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - ETA: 0s - loss: 0.4103 - accuracy: 0.8169\n",
      "Epoch 5: val_accuracy improved from 0.73750 to 0.74167, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "85/85 [==============================] - 502s 6s/step - loss: 0.4103 - accuracy: 0.8169 - val_loss: 0.9349 - val_accuracy: 0.7417 - lr: 1.0000e-04\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - ETA: 0s - loss: 0.3692 - accuracy: 0.8360\n",
      "Epoch 6: val_accuracy improved from 0.74167 to 0.77917, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "85/85 [==============================] - 500s 6s/step - loss: 0.3692 - accuracy: 0.8360 - val_loss: 0.8960 - val_accuracy: 0.7792 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.3137 - accuracy: 0.8618\n",
      "Epoch 7: val_accuracy improved from 0.77917 to 0.79167, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "85/85 [==============================] - 499s 6s/step - loss: 0.3137 - accuracy: 0.8618 - val_loss: 0.7936 - val_accuracy: 0.7917 - lr: 5.0000e-05\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.8706\n",
      "Epoch 8: val_accuracy did not improve from 0.79167\n",
      "85/85 [==============================] - 503s 6s/step - loss: 0.3082 - accuracy: 0.8706 - val_loss: 0.8095 - val_accuracy: 0.7917 - lr: 5.0000e-05\n",
      "Epoch 9/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.2815 - accuracy: 0.8794\n",
      "Epoch 9: val_accuracy improved from 0.79167 to 0.83333, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "85/85 [==============================] - 504s 6s/step - loss: 0.2815 - accuracy: 0.8794 - val_loss: 0.6681 - val_accuracy: 0.8333 - lr: 5.0000e-05\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.8838\n",
      "Epoch 10: val_accuracy did not improve from 0.83333\n",
      "85/85 [==============================] - 506s 6s/step - loss: 0.2691 - accuracy: 0.8838 - val_loss: 0.7596 - val_accuracy: 0.8167 - lr: 5.0000e-05\n",
      "Epoch 11/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.8904\n",
      "Epoch 11: val_accuracy improved from 0.83333 to 0.84583, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "85/85 [==============================] - 504s 6s/step - loss: 0.2652 - accuracy: 0.8904 - val_loss: 0.6484 - val_accuracy: 0.8458 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9059\n",
      "Epoch 12: val_accuracy improved from 0.84583 to 0.85000, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "85/85 [==============================] - 503s 6s/step - loss: 0.2278 - accuracy: 0.9059 - val_loss: 0.5389 - val_accuracy: 0.8500 - lr: 5.0000e-05\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9096\n",
      "Epoch 13: val_accuracy did not improve from 0.85000\n",
      "85/85 [==============================] - 501s 6s/step - loss: 0.2333 - accuracy: 0.9096 - val_loss: 0.5905 - val_accuracy: 0.8417 - lr: 5.0000e-05\n",
      "Epoch 14/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.9051\n",
      "Epoch 14: val_accuracy improved from 0.85000 to 0.86667, saving model to best_model_fold1_bigru_mhattn_enhanced.h5\n",
      "85/85 [==============================] - 500s 6s/step - loss: 0.2219 - accuracy: 0.9051 - val_loss: 0.4925 - val_accuracy: 0.8667 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuquangdang/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9279"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, TimeDistributed, GRU, Dropout, Dense, GlobalAveragePooling2D, \n",
    "    Concatenate, BatchNormalization, Bidirectional, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "    )\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuration - Enhanced\n",
    "video_keys = list(video_dict.keys())\n",
    "video_labels = [labels[k] for k in video_keys]\n",
    "\n",
    "img_size = (224, 224)\n",
    "batch_size = 16  # Reduced for better generalization\n",
    "epochs = 100  # Increased for better convergence\n",
    "n_splits = 5\n",
    "sequence_len = 30  # Increased to capture more temporal information\n",
    "results = []\n",
    "all_histories = []\n",
    "\n",
    "# Normalize CSV features with robust scaling\n",
    "print(\"🔧 Computing CSV feature statistics for normalization...\")\n",
    "all_csv_features = []\n",
    "for key in video_keys:\n",
    "    frames = video_dict[key][:sequence_len]\n",
    "    for path in frames:\n",
    "        filename = os.path.basename(path)\n",
    "        if filename in openface_features:\n",
    "            all_csv_features.append(openface_features[filename])\n",
    "\n",
    "# Debug: Check if we have any features\n",
    "print(f\"📊 Found {len(all_csv_features)} CSV features from {len(video_keys)} videos\")\n",
    "if len(all_csv_features) == 0:\n",
    "    print(\"⚠️ WARNING: No matching CSV features found!\")\n",
    "    print(f\"   Sample video filename: {os.path.basename(list(video_dict.values())[0][0])}\")\n",
    "    print(f\"   Sample CSV key: {list(openface_features.keys())[0]}\")\n",
    "    raise ValueError(\"No CSV features found - check filename matching!\")\n",
    "\n",
    "csv_scaler = StandardScaler()\n",
    "csv_scaler.fit(np.array(all_csv_features))  # Convert to numpy array\n",
    "print(f\"✅ CSV feature normalization fitted on {len(all_csv_features)} samples\")\n",
    "\n",
    "# Enhanced Data generator with stronger augmentation\n",
    "class VideoSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, video_keys, video_dict, labels, batch_size, img_size, sequence_len=15, augment=False):\n",
    "        self.video_keys = video_keys\n",
    "        self.video_dict = video_dict\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.sequence_len = sequence_len\n",
    "        self.augment = augment\n",
    "        # Enhanced augmentation for better regularization\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20 if augment else 0,\n",
    "            width_shift_range=0.15 if augment else 0,\n",
    "            height_shift_range=0.15 if augment else 0,\n",
    "            zoom_range=0.15 if augment else 0,\n",
    "            horizontal_flip=augment,\n",
    "            brightness_range=[0.8, 1.2] if augment else None,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.video_keys) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_keys = self.video_keys[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_X_img, batch_X_csv, batch_y = [], [], []\n",
    "\n",
    "        for key in batch_keys:\n",
    "            frames = self.video_dict[key][:self.sequence_len]\n",
    "            imgs = []\n",
    "            csv_feats = []\n",
    "            \n",
    "            for path in frames:\n",
    "                # Load and process image\n",
    "                img = cv2.imread(path)\n",
    "                img = cv2.resize(img, self.img_size)\n",
    "                img = self.datagen.random_transform(img) if self.augment else img\n",
    "                img = img.astype('float32') / 255.0\n",
    "                imgs.append(img)\n",
    "                \n",
    "                # Load CSV features and normalize (1-to-1 mapping)\n",
    "                filename = os.path.basename(path)\n",
    "                if filename in openface_features:\n",
    "                    csv_feat = openface_features[filename]\n",
    "                    csv_feat = csv_scaler.transform(csv_feat.reshape(1, -1))[0]\n",
    "                else:\n",
    "                    csv_feat = np.zeros(csv_feature_dim, dtype='float32')\n",
    "                csv_feats.append(csv_feat)\n",
    "            \n",
    "            # Pad sequences if needed\n",
    "            while len(imgs) < self.sequence_len:\n",
    "                imgs.append(np.zeros((*self.img_size, 3), dtype='float32'))\n",
    "                csv_feats.append(np.zeros(csv_feature_dim, dtype='float32'))\n",
    "            \n",
    "            batch_X_img.append(imgs)\n",
    "            batch_X_csv.append(csv_feats)\n",
    "            batch_y.append(self.labels[key])\n",
    "\n",
    "        return [np.array(batch_X_img), np.array(batch_X_csv)], np.array(batch_y)\n",
    "\n",
    "# Enhanced BiGRU + Multi-Head Self-Attention Model\n",
    "def build_model(sequence_len, img_size, csv_dim=674):\n",
    "    # MobileNet branch with more trainable layers\n",
    "    base_cnn = MobileNetV2(input_shape=(*img_size, 3), include_top=False, weights='imagenet')\n",
    "    base_cnn.trainable = True\n",
    "    # Fine-tune more layers for better feature extraction\n",
    "    for layer in base_cnn.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    cnn_out = GlobalAveragePooling2D()(base_cnn.output)\n",
    "    cnn_model = Model(inputs=base_cnn.input, outputs=cnn_out)\n",
    "    \n",
    "    # Image sequence input\n",
    "    input_seq_img = Input(shape=(sequence_len, *img_size, 3), name='image_input')\n",
    "    x_img = TimeDistributed(cnn_model)(input_seq_img)\n",
    "    \n",
    "    # CSV features input\n",
    "    input_seq_csv = Input(shape=(sequence_len, csv_dim), name='csv_input')\n",
    "    \n",
    "    # BatchNormalization before fusion\n",
    "    x_img = BatchNormalization(name='bn_mobilenet')(x_img)\n",
    "    x_csv = BatchNormalization(name='bn_csv')(input_seq_csv)\n",
    "    \n",
    "    # Concatenate features (1-to-1 fusion)\n",
    "    x_combined = Concatenate(axis=-1, name='feature_fusion')([x_img, x_csv])\n",
    "    \n",
    "    # Enhanced BiGRU layers with more capacity\n",
    "    x = Bidirectional(GRU(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, name='bigru_1'))(x_combined)\n",
    "    x = LayerNormalization(name='ln_bigru_1')(x)\n",
    "    x = Dropout(0.3, name='dropout_1')(x)\n",
    "    \n",
    "    x = Bidirectional(GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, name='bigru_2'))(x)\n",
    "    x = LayerNormalization(name='ln_bigru_2')(x)\n",
    "    x = Dropout(0.3, name='dropout_2')(x)\n",
    "    \n",
    "    # Enhanced Multi-Head Self-Attention with more heads\n",
    "    x_norm = LayerNormalization(name='ln_before_attention')(x)\n",
    "    attn_output = MultiHeadAttention(\n",
    "        num_heads=8,  # Increased from 4 to 8 for better pattern capture\n",
    "        key_dim=64,   # Increased from 32 to 64\n",
    "        dropout=0.1,\n",
    "        name='multi_head_attention'\n",
    "    )(x_norm, x_norm)\n",
    "    \n",
    "    # Residual connection + Layer normalization\n",
    "    x = tf.keras.layers.Add(name='residual_connection')([x, attn_output])\n",
    "    x = LayerNormalization(name='ln_after_attention')(x)\n",
    "    x = Dropout(0.2, name='dropout_attention_1')(x)\n",
    "    \n",
    "    # Additional attention layer for deeper understanding\n",
    "    x_norm2 = LayerNormalization(name='ln_before_attention_2')(x)\n",
    "    attn_output2 = MultiHeadAttention(\n",
    "        num_heads=4,\n",
    "        key_dim=32,\n",
    "        dropout=0.1,\n",
    "        name='multi_head_attention_2'\n",
    "    )(x_norm2, x_norm2)\n",
    "    \n",
    "    x = tf.keras.layers.Add(name='residual_connection_2')([x, attn_output2])\n",
    "    x = LayerNormalization(name='ln_after_attention_2')(x)\n",
    "    \n",
    "    # Global average pooling over time dimension\n",
    "    x = GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "    x = Dropout(0.3, name='dropout_attention_2')(x)\n",
    "    \n",
    "    # Enhanced classification layers with more capacity\n",
    "    x = Dense(128, activation='relu', name='dense_1')(x)\n",
    "    x = BatchNormalization(name='bn_dense_1')(x)\n",
    "    x = Dropout(0.4, name='dropout_3')(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu', name='dense_2')(x)\n",
    "    x = BatchNormalization(name='bn_dense_2')(x)\n",
    "    x = Dropout(0.3, name='dropout_4')(x)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=[input_seq_img, input_seq_csv], outputs=output, name='BiGRU_MultiHeadAttn')\n",
    "    return model\n",
    "\n",
    "# HMM post-processing (SAME NAME, unchanged)\n",
    "def hmm_postprocess(pred_probs, y_true, n_states=2):\n",
    "    pred_probs = pred_probs.reshape(-1, 1)\n",
    "    hmm = GaussianHMM(n_components=n_states, covariance_type=\"diag\", n_iter=100)\n",
    "    hmm.fit(pred_probs)\n",
    "    hidden_states = hmm.predict(pred_probs)\n",
    "\n",
    "    mapping = {}\n",
    "    for state in np.unique(hidden_states):\n",
    "        indices = [i for i in range(len(hidden_states)) if hidden_states[i] == state]\n",
    "        state_labels = [y_true[i] for i in indices]\n",
    "        if len(state_labels) > 0:\n",
    "            mapped_label = mode(state_labels, keepdims=True).mode[0]\n",
    "        else:\n",
    "            mapped_label = 0\n",
    "        mapping[state] = mapped_label\n",
    "\n",
    "    hmm_labels = np.array([mapping[s] for s in hidden_states])\n",
    "    return hmm_labels\n",
    "\n",
    "# Training K-Fold\n",
    "print(\"\\n🚀 Starting Enhanced BiGRU + Multi-Head Self-Attention Training:\")\n",
    "print(\"   ✅ Bidirectional GRU (256 + 128 units) with recurrent dropout\")\n",
    "print(\"   ✅ Dual Multi-Head Self-Attention (8 heads + 4 heads)\")\n",
    "print(\"   ✅ Layer Normalization after each major block\")\n",
    "print(\"   ✅ Enhanced data augmentation\")\n",
    "print(\"   ✅ Sequence length: 15 frames\")\n",
    "print(\"   ✅ MobileNet (last 30 layers trainable)\")\n",
    "print(\"   ✅ Stronger regularization (dropout 0.3-0.4)\")\n",
    "print(\"   ✅ HMM post-processing\\n\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (trainval_idx, test_idx) in enumerate(skf.split(video_keys, video_labels), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📊 Fold {fold}/{n_splits} - Enhanced BiGRU_MultiHeadAttn\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    trainval_keys = [video_keys[i] for i in trainval_idx]\n",
    "    test_keys = [video_keys[i] for i in test_idx]\n",
    "\n",
    "    y_trainval = [labels[k] for k in trainval_keys]\n",
    "    train_keys, val_keys = train_test_split(trainval_keys, test_size=0.15, stratify=y_trainval, random_state=fold)\n",
    "\n",
    "    train_gen = VideoSequence(train_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=True)\n",
    "    val_gen = VideoSequence(val_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=False)\n",
    "    test_gen = VideoSequence(test_keys, video_dict, labels, batch_size, img_size, sequence_len, augment=False)\n",
    "\n",
    "    model = build_model(sequence_len, img_size, csv_feature_dim)\n",
    "    \n",
    "    # Enhanced optimizer with learning rate warmup\n",
    "    initial_lr = 1e-4\n",
    "    model.compile(\n",
    "        optimizer=Adamax(learning_rate=initial_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📈 Enhanced Model Architecture:\")\n",
    "    print(f\"   - Model: BiGRU (256+128) + Dual Attention (8+4 heads)\")\n",
    "    print(f\"   - Total params: {model.count_params():,}\")\n",
    "    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    print(f\"   - Trainable params: {trainable_params:,}\")\n",
    "    print(f\"   - Sequence length: {sequence_len}\")\n",
    "    print(f\"   - Batch size: {batch_size}\")\n",
    "    print(f\"   - Initial learning rate: {initial_lr}\")\n",
    "\n",
    "    model_path = f\"best_model_fold{fold}_bigru_mhattn_enhanced.h5\"\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
    "\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=epochs,\n",
    "                        callbacks=[checkpoint, earlystop, reduce_lr], verbose=1)\n",
    "    all_histories.append(history.history)\n",
    "\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    y_true = [labels[k] for k in test_keys]\n",
    "    y_pred_prob = model.predict(test_gen).ravel()\n",
    "    y_hmm_pred = hmm_postprocess(y_pred_prob, y_true)\n",
    "\n",
    "    results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy_score(y_true, y_hmm_pred),\n",
    "        'precision': precision_score(y_true, y_hmm_pred),\n",
    "        'recall': recall_score(y_true, y_hmm_pred),\n",
    "        'f1': f1_score(y_true, y_hmm_pred),\n",
    "        'auc': roc_auc_score(y_true, y_pred_prob)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n✅ Fold {fold} Results (Enhanced BiGRU_MultiHeadAttn):\")\n",
    "    print(f\"   Accuracy:  {results[-1]['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {results[-1]['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {results[-1]['recall']:.4f}\")\n",
    "    print(f\"   F1 Score:  {results[-1]['f1']:.4f}\")\n",
    "    print(f\"   AUC:       {results[-1]['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"📊 FINAL RESULTS - Enhanced BiGRU_MultiHeadAttn Model\")\n",
    "print(f\"{'='*60}\")\n",
    "for r in results:\n",
    "    print(f\"Fold {r['fold']}: Acc={r['accuracy']:.4f}, Prec={r['precision']:.4f}, Rec={r['recall']:.4f}, F1={r['f1']:.4f}, AUC={r['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n📊 Average Metrics:\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.mean(numeric_only=True))\n",
    "\n",
    "# Calculate CV metrics\n",
    "accuracy_mean = results_df['accuracy'].mean()\n",
    "accuracy_std = results_df['accuracy'].std()\n",
    "accuracy_range = results_df['accuracy'].max() - results_df['accuracy'].min()\n",
    "accuracy_cv_percent = (accuracy_std / accuracy_mean) * 100\n",
    "\n",
    "print(f\"\\n📈 Cross-Validation Stability:\")\n",
    "print(f\"   Mean Accuracy: {accuracy_mean:.4f}\")\n",
    "print(f\"   Std Deviation: {accuracy_std:.4f}\")\n",
    "print(f\"   Range: {accuracy_range:.4f}\")\n",
    "print(f\"   CV%: {accuracy_cv_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfa766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T01:45:10.343234Z",
     "start_time": "2025-06-17T01:45:10.278158Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"📊 Kết quả trung bình:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48802125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T07:31:56.035350Z",
     "start_time": "2025-06-14T07:31:50.121545Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, hist in enumerate(all_histories, 1):\n",
    "    plt.figure()\n",
    "    plt.plot(hist['accuracy'], label='Train Acc')\n",
    "    plt.plot(hist['val_accuracy'], label='Val Acc')\n",
    "    plt.title(f'Fold {i} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(hist['loss'], label='Train Loss')\n",
    "    plt.plot(hist['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Fold {i} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc4c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T07:32:05.949380Z",
     "start_time": "2025-06-14T07:32:05.516519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save mô hình fold cuối cùng\n",
    "model.save(\"mobilenetv2_hmm_faceplus_final.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9581629f-92c8-4d93-88f5-8c57f3197a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"📊 Kết quả trung bình:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167c3f6-2212-4cde-ab07-1ae1449219bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Giả sử results đã có và bạn đã tạo results_df\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Tính các chỉ số\n",
    "accuracy_mean = results_df['accuracy'].mean()\n",
    "accuracy_std = results_df['accuracy'].std()  # dùng sample std (chia cho n-1)\n",
    "accuracy_range = results_df['accuracy'].max() - results_df['accuracy'].min()\n",
    "accuracy_cv_percent = (accuracy_std / accuracy_mean) * 100\n",
    "\n",
    "# In kết quả\n",
    "print(\"📊 Kết quả trung bình:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "\n",
    "print(f\"\\n✅ CV Accuracy (Mean Accuracy): {accuracy_mean:.4f}\")\n",
    "print(f\"📈 Range Accuracy: {accuracy_range:.4f}\")\n",
    "print(f\"📉 Accuracy CV% (std/mean): {accuracy_cv_percent:.2f}%\")\n",
    "\n",
    "# Hiển thị bảng kết quả nếu cần\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4efdd-8ae0-4309-9f39-3a151116abb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
